import gradio as gr
import cv2
import numpy as np
from PIL import Image
import os
from datetime import datetime
from jiwer import cer, wer

# Import c√°c l·ªõp t·ª´ d·ª± √°n c·ªßa b·∫°n
from mltu.inferenceModel import OnnxInferenceModel
from mltu.utils.text_utils import ctc_decoder
from mltu.transformers import ImageResizer
from mltu.configs import BaseModelConfigs


class ImageToWordModel(OnnxInferenceModel):
    def __init__(self, char_list, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.char_list = char_list

    def predict(self, image):
        # 1. Resize gi·ªØ nguy√™n t·ªâ l·ªá theo chu·∫©n c·ªßa model
        image = ImageResizer.resize_maintaining_aspect_ratio(
            image, *self.input_shapes[0][1:3][::-1]
        )

        # ƒê·∫¢M B·∫¢O 3 K√äNH M√ÄU: Tr√°nh l·ªói Rank cho ONNX
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
        elif image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)

        # 2. T·∫°o Rank 4 (1, H, W, C)
        image_pred = np.expand_dims(image, axis=0).astype(np.float32)

        # 3. Ch·∫°y d·ª± ƒëo√°n
        preds = self.model.run(self.output_names, {self.input_names[0]: image_pred})[0]

        # 4. T√≠nh to√°n Confidence Score th·ª±c t·∫ø t·ª´ l·ªõp Softmax
        softmax_preds = np.exp(preds) / np.sum(np.exp(preds), axis=-1, keepdims=True)
        max_probs = np.max(softmax_preds, axis=-1)[0]
        confidence = np.mean(max_probs) * 100

        # 5. Gi·∫£i m√£ vƒÉn b·∫£n b·∫±ng CTC
        text = ctc_decoder(preds, self.char_list)[0]

        return text, confidence


# Load c·∫•u h√¨nh
config_path = "models/model_demo/configs.yaml"
configs = BaseModelConfigs.load(config_path)

# Kh·ªüi t·∫°o model
model = ImageToWordModel(
    model_path=configs.model_path,
    char_list=configs.vocab
)

# L∆∞u l·ªãch s·ª≠
history = []


def recognize_handwriting(image, ground_truth=None):
    try:
        if image is None:
            return "‚ö†Ô∏è Vui l√≤ng cung c·∫•p ·∫£nh!", "", "", ""

        # --- B∆Ø·ªöC 1: X·ª¨ L√ù ƒê·ªäNH D·∫†NG ƒê·∫¶U V√ÄO ---
        if isinstance(image, dict):
            image = image.get("composite", image.get("background"))

        if isinstance(image, Image.Image):
            image = np.array(image)

        if not isinstance(image, np.ndarray):
            return "‚ùå ƒê·ªãnh d·∫°ng ·∫£nh kh√¥ng h·ª£p l·ªá!", "", "", ""

        # Chu·∫©n h√≥a m√†u s·∫Øc
        if len(image.shape) == 3:
            if image.shape[2] == 4:
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            elif image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        elif len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)

        # --- B∆Ø·ªöC 2: D·ª∞ ƒêO√ÅN (S·ª¨A L·ªñI T·∫†I ƒê√ÇY) ---
        # Ph·∫£i t√°ch k·∫øt qu·∫£ ra l√†m 2 bi·∫øn ri√™ng bi·ªát
        prediction_text, confidence_score = model.predict(image)

        # --- B∆Ø·ªöC 3: T√çNH TO√ÅN CER/WER TH·∫¨T ---
        if ground_truth and ground_truth.strip() != "":
            val_cer = cer(ground_truth.strip(), prediction_text)
            val_wer = wer(ground_truth.strip(), prediction_text)

            result = f"‚úÖ **K·∫øt qu·∫£:** {prediction_text}"
            confidence_display = f"üéØ **ƒê·ªô tin c·∫≠y:** {confidence_score:.2f}%"
            metrics = f"üìä **Metrics th·ª±c t·∫ø:**\n- CER: {val_cer:.2%}\n- WER: {val_wer:.2%}"
        else:
            result = f"‚úÖ **K·∫øt qu·∫£:** {prediction_text}"
            confidence_display = f"üéØ **ƒê·ªô tin c·∫≠y:** {confidence_score:.2f}%"
            metrics = "üìä **Metrics:** Nh·∫≠p 'Ground Truth' ƒë·ªÉ xem k·∫øt qu·∫£"

        # C·∫≠p nh·∫≠t l·ªãch s·ª≠
        timestamp = datetime.now().strftime("%H:%M:%S")
        history.insert(0, f"[{timestamp}] {prediction_text} ({confidence_score:.1f}%)")
        history_text = "\n\n".join(history[:5])

        return result, confidence_display, metrics, history_text

    except Exception as e:
        return f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}", "", "", ""


def clear_all():
    return None, "", "", "", ""


# CSS trang tr√≠
custom_css = """
#main_container { max-width: 1400px; margin: auto; }
.gradio-container { font-family: 'Inter', sans-serif; }
#title { text-align: center; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-size: 3em; font-weight: bold; margin-bottom: 10px; }
#subtitle { text-align: center; color: #666; font-size: 1.2em; margin-bottom: 30px; }
"""

with gr.Blocks(css=custom_css) as demo:
    gr.HTML("""
        <div id="title">‚úçÔ∏è Handwriting Recognition AI</div>
        <div id="subtitle">Upload an image or draw your handwritten text to convert it into digital text</div>
    """)

    with gr.Row(elem_id="main_container"):
        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.Tab("üì§ Upload Image"):
                    image_input = gr.Image(label="Ch·ªçn ·∫£nh", type="pil", height=300)
                    ground_truth_input = gr.Textbox(label="Ground Truth (ƒê·ªëi chi·∫øu ƒë√∫ng/sai)",
                                                    placeholder="V√≠ d·ª•: Hello")

                with gr.Tab("‚úèÔ∏è Draw Text"):
                    sketch_input = gr.Sketchpad(label="V·∫Ω tay", type="pil", height=400,
                                                brush=gr.Brush(colors=["#000000"], default_size=3))

            with gr.Row():
                recognize_btn = gr.Button("üîç Nh·∫≠n di·ªán", variant="primary", size="lg")
                clear_btn = gr.Button("üóëÔ∏è X√≥a h·∫øt", variant="secondary", size="lg")

        with gr.Column(scale=1):
            result_output = gr.Markdown(label="K·∫øt qu·∫£", value="T·∫£i ·∫£nh l√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu!")
            confidence_output = gr.Markdown(label="ƒê·ªô tin c·∫≠y")
            metrics_output = gr.Markdown(label="Ch·ªâ s·ªë l·ªói")
            gr.Markdown("### üìú L·ªãch s·ª≠")
            history_output = gr.Textbox(label="5 l·∫ßn g·∫ßn nh·∫•t", lines=8, interactive=False)

    with gr.Accordion("‚ÑπÔ∏è Th√¥ng tin m√¥ h√¨nh", open=False):
        gr.Markdown(f"""
        ### Model Configuration
        - **Vocabulary:** {len(configs.vocab)} k√Ω t·ª±
        - **Input Size:** {configs.width}x{configs.height}
        - **Architecture:** ResNet-CNN + Bi-LSTM + CTC
        """)

    # G√°n s·ª± ki·ªán
    recognize_btn.click(
        fn=lambda img_u, img_s, gt: recognize_handwriting(img_u if img_u else img_s, gt),
        inputs=[image_input, sketch_input, ground_truth_input],
        outputs=[result_output, confidence_output, metrics_output, history_output]
    )

    clear_btn.click(
        fn=clear_all,
        inputs=[],
        outputs=[image_input, result_output, confidence_output, metrics_output, history_output]
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7860, inbrowser=True, theme=gr.themes.Soft())