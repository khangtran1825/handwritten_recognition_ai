import gradio as gr
import cv2
import numpy as np
from PIL import Image
import os
from datetime import datetime
from jiwer import cer, wer

# Import c√°c l·ªõp t·ª´ d·ª± √°n c·ªßa b·∫°n
from mltu.inferenceModel import OnnxInferenceModel
from mltu.utils.text_utils import ctc_decoder
from mltu.transformers import ImageResizer
from mltu.configs import BaseModelConfigs


class ImageToWordModel(OnnxInferenceModel):
    def __init__(self, char_list, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.char_list = char_list

    def predict(self, image):
        # 1-3. Preprocessing v√† prediction
        image = ImageResizer.resize_maintaining_aspect_ratio(
            image, *self.input_shapes[0][1:3][::-1]
        )
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
        elif image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)

        image_pred = np.expand_dims(image, axis=0).astype(np.float32)
        preds = self.model.run(self.output_names, {self.input_names[0]: image_pred})[0]

        # 4. Gi·∫£i m√£ text
        text = ctc_decoder(preds, self.char_list)[0]

        print(f"\n=== KI·ªÇM TRA ƒê·ªäNH D·∫†NG PREDS ===")
        print(f"Preds shape: {preds.shape}")
        print(f"Preds range: min={np.min(preds):.4f}, max={np.max(preds):.4f}")
        print(f"Sum across vocab (first timestep): {np.sum(preds[0, 0, :]):.4f}")

        # Ki·ªÉm tra xem preds ƒë√£ l√† x√°c su·∫•t ch∆∞a
        # N·∫øu sum ‚âà 1.0 ‚Üí ƒë√£ l√† softmax
        # N·∫øu sum kh√°c 1 nhi·ªÅu ‚Üí l√† logits ho·∫∑c log-probs
        sum_first_timestep = np.sum(preds[0, 0, :])

        if 0.99 < sum_first_timestep < 1.01:
            print("‚úÖ PREDS ƒê√É L√Ä SOFTMAX PROBABILITIES!")
            softmax_preds = preds  # Kh√¥ng c·∫ßn t√≠nh l·∫°i
        elif np.min(preds) < 0:
            print("‚úÖ PREDS L√Ä LOG-PROBABILITIES (√¢m) ‚Üí D√πng exp()")
            softmax_preds = np.exp(preds)  # Log-probs ‚Üí probs
        else:
            print("‚úÖ PREDS L√Ä LOGITS (d∆∞∆°ng) ‚Üí D√πng softmax")
            preds_shifted = preds - np.max(preds, axis=-1, keepdims=True)
            softmax_preds = np.exp(preds_shifted) / np.sum(np.exp(preds_shifted), axis=-1, keepdims=True)

        # L·∫•y max prob m·ªói timestep
        max_probs = np.max(softmax_preds[0], axis=-1)
        print(f"\nSau x·ª≠ l√Ω:")
        print(
            f"Max probs stats: min={np.min(max_probs):.4f}, max={np.max(max_probs):.4f}, mean={np.mean(max_probs):.4f}")

        # L·∫•y predicted classes
        predicted_indices = np.argmax(softmax_preds[0], axis=-1)
        blank_index = len(self.char_list)

        # T√≠nh confidence t·ª´ non-blank tokens
        non_blank_probs = []
        for t, idx in enumerate(predicted_indices):
            if idx != blank_index:
                prob = softmax_preds[0, t, idx]
                non_blank_probs.append(prob)

        # CODE M·ªöI
        if len(non_blank_probs) > 0:
            # B∆∞·ªõc 1: Chuy·ªÉn c√°c x√°c su·∫•t sang kh√¥ng gian Log ƒë·ªÉ t√≠nh to√°n an to√†n
            # np.maximum(..., 1e-9) ƒë·ªÉ tr√°nh l·ªói log(0)
            log_probs = np.log(np.maximum(non_blank_probs, 1e-9))

            # B∆∞·ªõc 2: T√≠nh trung b√¨nh c·ªông c·ªßa c√°c Log
            mean_log = np.mean(log_probs)

            # B∆∞·ªõc 3: D√πng h√†m Exp ƒë·ªÉ ƒë∆∞a v·ªÅ gi√° tr·ªã x√°c su·∫•t g·ªëc (ƒë√¢y ch√≠nh l√† Geometric Mean)
            geometric_mean = np.exp(mean_log)

            confidence = geometric_mean * 100
        else:
            confidence = 0.0

        print("=" * 40 + "\n")

        return text, confidence


# Load c·∫•u h√¨nh
config_path = "models/model_demo/configs.yaml"
configs = BaseModelConfigs.load(config_path)

# Kh·ªüi t·∫°o model
model = ImageToWordModel(
    model_path=configs.model_path,
    char_list=configs.vocab
)

# L∆∞u l·ªãch s·ª≠
history = []


def recognize_handwriting(image, ground_truth=None):
    try:
        if image is None:
            return "‚ö†Ô∏è Vui l√≤ng cung c·∫•p ·∫£nh!", "", "", ""

        # --- B∆Ø·ªöC 1: X·ª¨ L√ù ƒê·ªäNH D·∫†NG ƒê·∫¶U V√ÄO ---
        if isinstance(image, dict):
            image = image.get("composite", image.get("background"))

        if isinstance(image, Image.Image):
            image = np.array(image)

        if not isinstance(image, np.ndarray):
            return "‚ùå ƒê·ªãnh d·∫°ng ·∫£nh kh√¥ng h·ª£p l·ªá!", "", "", ""

        # Chu·∫©n h√≥a m√†u s·∫Øc
        if len(image.shape) == 3:
            if image.shape[2] == 4:
                image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)
            elif image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        elif len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)

        # --- B∆Ø·ªöC 2: D·ª∞ ƒêO√ÅN (S·ª¨A L·ªñI T·∫†I ƒê√ÇY) ---
        # Ph·∫£i t√°ch k·∫øt qu·∫£ ra l√†m 2 bi·∫øn ri√™ng bi·ªát
        prediction_text, confidence_score = model.predict(image)

        # --- B∆Ø·ªöC 3: T√çNH TO√ÅN CER/WER TH·∫¨T ---
        if ground_truth and ground_truth.strip() != "":
            val_cer = cer(ground_truth.strip(), prediction_text)
            val_wer = wer(ground_truth.strip(), prediction_text)

            result = f"‚úÖ **K·∫øt qu·∫£:** {prediction_text}"
            confidence_display = f"üéØ **ƒê·ªô tin c·∫≠y:** {confidence_score:.2f}%"
            metrics = f"üìä **Metrics th·ª±c t·∫ø:**\n- CER: {val_cer:.2%}\n- WER: {val_wer:.2%}"
        else:
            result = f"‚úÖ **K·∫øt qu·∫£:** {prediction_text}"
            confidence_display = f"üéØ **ƒê·ªô tin c·∫≠y:** {confidence_score:.2f}%"
            metrics = "üìä **Metrics:** Nh·∫≠p 'Ground Truth' ƒë·ªÉ xem k·∫øt qu·∫£"

        # C·∫≠p nh·∫≠t l·ªãch s·ª≠
        timestamp = datetime.now().strftime("%H:%M:%S")
        history.insert(0, f"[{timestamp}] {prediction_text} ({confidence_score:.1f}%)")
        history_text = "\n\n".join(history[:5])

        return result, confidence_display, metrics, history_text

    except Exception as e:
        return f"‚ùå L·ªói h·ªá th·ªëng: {str(e)}", "", "", ""


def clear_all():
    return None, "", "", "", ""


# CSS trang tr√≠
custom_css = """
#main_container { max-width: 1400px; margin: auto; }
.gradio-container { font-family: 'Inter', sans-serif; }
#title { text-align: center; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; font-size: 3em; font-weight: bold; margin-bottom: 10px; }
#subtitle { text-align: center; color: #666; font-size: 1.2em; margin-bottom: 30px; }
"""

with gr.Blocks(css=custom_css) as demo:
    gr.HTML("""
        <div id="title">‚úçÔ∏è Handwriting Recognition AI</div>
        <div id="subtitle">Upload an image or draw your handwritten text to convert it into digital text</div>
    """)

    with gr.Row(elem_id="main_container"):
        with gr.Column(scale=2):
            with gr.Tabs():
                with gr.Tab("üì§ Upload Image"):
                    image_input = gr.Image(label="Ch·ªçn ·∫£nh", type="pil", height=300)
                    ground_truth_input = gr.Textbox(label="Ground Truth (ƒê·ªëi chi·∫øu ƒë√∫ng/sai)",
                                                    placeholder="V√≠ d·ª•: Hello")

                with gr.Tab("‚úèÔ∏è Draw Text"):
                    sketch_input = gr.Sketchpad(label="V·∫Ω tay", type="pil", height=400,
                                                brush=gr.Brush(colors=["#000000"], default_size=3))

            with gr.Row():
                recognize_btn = gr.Button("üîç Nh·∫≠n di·ªán", variant="primary", size="lg")
                clear_btn = gr.Button("üóëÔ∏è X√≥a h·∫øt", variant="secondary", size="lg")

        with gr.Column(scale=1):
            result_output = gr.Markdown(label="K·∫øt qu·∫£", value="T·∫£i ·∫£nh l√™n ƒë·ªÉ b·∫Øt ƒë·∫ßu!")
            confidence_output = gr.Markdown(label="ƒê·ªô tin c·∫≠y")
            metrics_output = gr.Markdown(label="Ch·ªâ s·ªë l·ªói")
            gr.Markdown("### üìú L·ªãch s·ª≠")
            history_output = gr.Textbox(label="5 l·∫ßn g·∫ßn nh·∫•t", lines=8, interactive=False)

    with gr.Accordion("‚ÑπÔ∏è Th√¥ng tin m√¥ h√¨nh", open=False):
        gr.Markdown(f"""
        ### Model Configuration
        - **Vocabulary:** {len(configs.vocab)} k√Ω t·ª±
        - **Input Size:** {configs.width}x{configs.height}
        - **Architecture:** ResNet-CNN + Bi-LSTM + CTC
        """)

    # G√°n s·ª± ki·ªán
    recognize_btn.click(
        fn=lambda img_u, img_s, gt: recognize_handwriting(img_u if img_u else img_s, gt),
        inputs=[image_input, sketch_input, ground_truth_input],
        outputs=[result_output, confidence_output, metrics_output, history_output]
    )

    clear_btn.click(
        fn=clear_all,
        inputs=[],
        outputs=[image_input, result_output, confidence_output, metrics_output, history_output]
    )

if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7860, inbrowser=True, theme=gr.themes.Soft())